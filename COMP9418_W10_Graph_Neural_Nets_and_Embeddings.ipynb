{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b0ac21-c62d-42f5-a3de-5ad63ef2803c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9418/Week10/blob/main/COMP9418_W10_Graph_Neural_Nets_and_Embeddings.ipynb)\n",
    "\n",
    "# Graph Neural Nets and Embeddings\n",
    "\n",
    "**COMP9418 W10 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Notebook designed by Gustavo Batista\n",
    "- Last Update 31st July 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580203b-cbd3-4a81-8da2-208daffe0471",
   "metadata": {},
   "source": [
    "# Graph Neural Network Analysis on the Cora Dataset\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Graphs are a powerful representation for relational data, capturing the dependencies between entities (nodes) through their connections (edges). Traditional machine learning models typically ignore this structure, treating each sample independently. In contrast, **Graph Neural Networks (GNNs)** leverage both node features and graph structure to learn powerful representations for tasks like node classification, link prediction, and graph classification.\n",
    "\n",
    "In this tutorial, we will study the task of **node classification** using the **Cora citation network**, a widely used benchmark dataset in graph learning. Each node in the graph represents a scientific publication, with edges denoting citation links. Node features are derived from the text content, and the goal is to predict the research area (class) of each paper.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This tutorial will guide you through the complete pipeline for GNN-based node classification:\n",
    "\n",
    "- **Data Preparation**: Load and preprocess the Cora dataset, construct the graph, and create train/validation/test splits.\n",
    "- **Exploratory Data Analysis (EDA)**: Visualize class distributions and feature embeddings using t-SNE.\n",
    "- **Baseline Model**: Implement and evaluate a shallow encoder that learns node embeddings independently of graph structure.\n",
    "- **Node2Vec**: Generate node embeddings via biased random walks and evaluate their performance.\n",
    "- **Graph Convolutional Network (GCN)**: Implement a simple two-layer GCN and analyze its performance relative to the baselines.\n",
    "- **Performance Comparison**: Summarize the results across all models and visualize their accuracy.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Understand the differences between non-structural (MLP), structural (Node2Vec), and graph-convolution-based models.\n",
    "- Learn how random walks and convolutional propagation affect the learned embeddings.\n",
    "- Gain hands-on experience with TensorFlow-based implementations of GNNs from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ea492-538d-47d4-baaa-37e4082d60a9",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "This tutorial requires several Python libraries for scientific computing, visualization, and neural network training. If you are running this notebook on your machine and do not already have these libraries installed, you can install them via `pip`:\n",
    "\n",
    "\n",
    "```pip install numpy scipy matplotlib seaborn scikit-learn tensorflow```\n",
    "\n",
    "If you are using a managed environment like Google Colab, most of these packages are pre-installed. However, TensorFlow might require an upgrade to version 2.x using:\n",
    "\n",
    "```pip install --upgrade tensorflow```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd211d2-945f-4524-8bad-ca616757da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you do not want to use your libraries and\n",
    "# prefer to download our implementation from GitHub\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/UNSW-COMP9418/libraries/main/\"\n",
    "files = [\"Graph.py\"]\n",
    "\n",
    "for file in files:\n",
    "    url = base_url + file\n",
    "    print(f\"Downloading {file}...\")\n",
    "    urllib.request.urlretrieve(url, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58cb00-009d-43fc-8d9f-b7f1377db069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIBRARY IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, Input\n",
    "#from tensorflow.keras.optimizers.legacy import Adam               # Use this line if you have a M1/2/3/4 MacBook\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from Graph import Graph\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96a899-fd96-4e68-a30a-6fff4f84a10b",
   "metadata": {},
   "source": [
    "## Loading the Cora Dataset\n",
    "\n",
    "The Cora dataset is a citation network where:\n",
    "- Each **node** represents a scientific publication.\n",
    "- Each **edge** represents a citation link from one paper to another.\n",
    "- Each node is annotated with:\n",
    "  - A **feature vector** representing word occurrences from the paper’s text (TF-IDF).\n",
    "  - A **class label** indicating its topic.\n",
    "\n",
    "We will use the Planetoid version of the dataset, which is commonly used in graph learning benchmarks. If the dataset is not already present locally, we will download the necessary files from the GitHub repository maintained by the original authors.\n",
    "\n",
    "The dataset comes as several `.x`, `.y`, `.allx`, `.ally`, `.graph`, and `.test.index` files, which we’ll load and merge into a single feature matrix, label array, and adjacency list representation of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357df3d-95e4-4e95-a8dd-df268045aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOWNLOAD AND LOAD CORA DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def download_cora_dataset():\n",
    "    \"\"\"Download Cora dataset files from Planetoid repository.\"\"\"\n",
    "    url = \"https://github.com/kimiyoung/planetoid/raw/master/data\"\n",
    "    files = [\n",
    "        \"ind.cora.x\", \"ind.cora.tx\", \"ind.cora.allx\",\n",
    "        \"ind.cora.y\", \"ind.cora.ty\", \"ind.cora.ally\",\n",
    "        \"ind.cora.graph\", \"ind.cora.test.index\"\n",
    "    ]\n",
    "    os.makedirs(\"cora_data\", exist_ok=True)\n",
    "    \n",
    "    for filename in files:\n",
    "        full_path = os.path.join(\"cora_data\", filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(f\"{url}/{filename}\", full_path)\n",
    "        else:\n",
    "            print(f\"{filename} already exists.\")\n",
    "\n",
    "def load_pickle_file(filepath):\n",
    "    \"\"\"Load pickle file with latin1 encoding.\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "def load_cora_data():\n",
    "    \"\"\"Load and preprocess Cora dataset.\"\"\"\n",
    "    data_dir = \"cora_data\"\n",
    "\n",
    "    # Load dataset parts\n",
    "    x_train = load_pickle_file(f\"{data_dir}/ind.cora.x\")\n",
    "    x_test = load_pickle_file(f\"{data_dir}/ind.cora.tx\")\n",
    "    x_all = load_pickle_file(f\"{data_dir}/ind.cora.allx\")\n",
    "    y_train = load_pickle_file(f\"{data_dir}/ind.cora.y\")\n",
    "    y_test = load_pickle_file(f\"{data_dir}/ind.cora.ty\")\n",
    "    y_all = load_pickle_file(f\"{data_dir}/ind.cora.ally\")\n",
    "    graph = load_pickle_file(f\"{data_dir}/ind.cora.graph\")\n",
    "    test_indices = np.loadtxt(f\"{data_dir}/ind.cora.test.index\", dtype=int)\n",
    "\n",
    "    # Number of nodes in the graph\n",
    "    num_nodes = max(graph.keys()) + 1\n",
    "\n",
    "    # Assemble full feature matrix\n",
    "    X_features = sp.lil_matrix((num_nodes, x_all.shape[1]))\n",
    "    X_features[:x_all.shape[0]] = x_all\n",
    "    for i, idx in enumerate(test_indices):\n",
    "        X_features[idx] = x_test[i]\n",
    "\n",
    "    # Assemble full label matrix\n",
    "    Y_labels = np.zeros((num_nodes, y_all.shape[1]))\n",
    "    Y_labels[:y_all.shape[0]] = y_all\n",
    "    for i, idx in enumerate(test_indices):\n",
    "        Y_labels[idx] = y_test[i]\n",
    "\n",
    "    return X_features, Y_labels, graph, num_nodes\n",
    "\n",
    "# Trigger dataset download and loading\n",
    "print(\"=== LOADING CORA DATASET ===\")\n",
    "download_cora_dataset()\n",
    "X_features, Y_labels, graph_dict, num_nodes = load_cora_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c8ef0-41a7-4261-9367-5824c95c50a8",
   "metadata": {},
   "source": [
    "## Exploring the Raw Cora Dataset\n",
    "\n",
    "Before diving into preprocessing, let's explore the raw components of the Cora dataset to understand its structure:\n",
    "\n",
    "- `x`, `tx`, `allx`: Sparse feature matrices.\n",
    "- `y`, `ty`, `ally`: One-hot encoded label matrices.\n",
    "- `graph`: A dictionary mapping each node to a list of neighbors.\n",
    "- `test.index`: List of indices used as test set in the original split.\n",
    "\n",
    "We'll inspect:\n",
    "- The shape and type of the feature and label matrices.\n",
    "- An example entry from the graph structure.\n",
    "- What the label vectors and feature vectors look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc408e-e80a-45e7-b4c7-ac8121931788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect shapes and types\n",
    "print(f\"x_all shape: {X_features.shape}\")\n",
    "print(f\"y_all shape: {Y_labels.shape}\")\n",
    "print(f\"Number of nodes in graph: {len(graph_dict)}\")\n",
    "print(f\"Example node ID: {list(graph_dict.keys())[0]}\")\n",
    "print(f\"Neighbors of node 0: {graph_dict[0]}\")\n",
    "\n",
    "# Inspect node features (sparse vector)\n",
    "sample_node = 0\n",
    "print(f\"\\nNode {sample_node} feature vector (non-zero indices):\")\n",
    "print(np.nonzero(X_features[sample_node])[1])\n",
    "\n",
    "# Inspect one-hot encoded labels\n",
    "print(f\"\\nNode {sample_node} label (one-hot):\")\n",
    "print(Y_labels[sample_node])\n",
    "print(f\"Node {sample_node} label (class index): {np.argmax(Y_labels[sample_node])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43231cf-2840-4df5-a3ac-155d40427c04",
   "metadata": {},
   "source": [
    "## Visualizing Graph Structure and Labels\n",
    "\n",
    "Before we preprocess the dataset, let's visualize two important aspects:\n",
    "\n",
    "1. **Class Distribution**: How many nodes belong to each class?\n",
    "2. **Node Degree Distribution**: How many connections (edges) each node has?\n",
    "\n",
    "These give us insights into:\n",
    "- Whether the classification task is imbalanced.\n",
    "- Whether the graph is sparse or dense, and whether degree heterogeneity might affect learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704c68d-db74-45ca-8038-7fa92eb71316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Convert one-hot to class labels\n",
    "y_labels = np.argmax(Y_labels, axis=1)\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=y_labels, palette=\"tab10\")\n",
    "plt.title(\"Node Class Distribution\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute node degrees\n",
    "degrees = [len(neighbors) for node, neighbors in graph_dict.items()]\n",
    "\n",
    "# Plot degree distribution with log-scaled x-axis\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(degrees, bins=np.logspace(np.log10(1), np.log10(max(degrees)+1), 30),\n",
    "         color='gray', edgecolor='black')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Node Degree Distribution (Log Scale)\")\n",
    "plt.xlabel(\"Degree (log scale)\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fcc4b-0d9b-4263-a108-329875454f34",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "To prepare the dataset for learning, we perform the following steps:\n",
    "\n",
    "1. **Feature Normalization**: Normalize node feature vectors using L2 norm. This ensures uniform scale across nodes.\n",
    "2. **Label Conversion**: Convert one-hot encoded labels into integer class IDs.\n",
    "3. **Adjacency Matrix Construction**: Convert the dictionary-based graph into a sparse adjacency matrix.\n",
    "4. **Symmetrization**: Since the citation graph is undirected, we ensure the adjacency matrix is symmetric.\n",
    "\n",
    "This preprocessing is necessary for both baseline models and graph neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd465efd-1e09-48ea-ac86-d292739472dc",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the scikit-learn function `normalize` to normalise the node features, use the argument `norm='L2'`. Also, use numpy's `argmax` method to convert one-hot encoded labels into integer class IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d0973-c33e-4f88-9e4d-ca4b3dbf8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_features, Y_labels, graph):\n",
    "    \"\"\"Normalize features, convert labels, and build sparse adjacency matrix.\"\"\"\n",
    "    \n",
    "    # Normalize node features (L2 norm per row)\n",
    "    X_normalized = ...                             # TODO: feature normalisation, 1 line\n",
    "    \n",
    "    # Convert one-hot labels to integers\n",
    "    y_labels = ...                                 # TODO: class IDs from one-hot encoding\n",
    "    \n",
    "    # Build adjacency matrix\n",
    "    num_nodes = X_normalized.shape[0]\n",
    "    adj_matrix = sp.lil_matrix((num_nodes, num_nodes))\n",
    "    for src, neighbors in graph.items():\n",
    "        for dst in neighbors:\n",
    "            adj_matrix[src, dst] = 1\n",
    "            adj_matrix[dst, src] = 1  # Make symmetric (undirected graph)\n",
    "    \n",
    "    return X_normalized, y_labels, adj_matrix\n",
    "\n",
    "# Run preprocessing\n",
    "X_normalized, y_labels, adj_matrix = preprocess_data(X_features, Y_labels, graph_dict)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Feature matrix shape: {X_normalized.shape}\")\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "print(f\"Number of edges: {adj_matrix.nnz // 2}\")  # undirecteda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffdaad-6bb2-4c88-a874-15ddae59e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_adjacency(adj_matrix, directed=False):\n",
    "    \"\"\"Build custom Graph object from adjacency matrix.\"\"\"\n",
    "    G = Graph()\n",
    "    adj_dense = adj_matrix.todense() if hasattr(adj_matrix, 'todense') else adj_matrix\n",
    "    \n",
    "    for i in range(adj_dense.shape[0]):\n",
    "        for j in range(adj_dense.shape[1]):\n",
    "            if adj_dense[i, j] != 0:\n",
    "                G.add_edge(i, j, directed=directed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d266340-ffca-4eb3-b629-080e5e301171",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_object = build_graph_from_adjacency(adj_matrix, directed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3b4d7-075e-4aba-ba78-c96d2812e880",
   "metadata": {},
   "source": [
    "## Creating Train, Validation, and Test Splits\n",
    "\n",
    "To evaluate our models properly, we divide the dataset into three disjoint sets:\n",
    "\n",
    "- **Training set**: Used to fit the model parameters.\n",
    "- **Validation set**: Used to tune hyperparameters and monitor overfitting.\n",
    "- **Test set**: Used to evaluate final model performance on unseen data.\n",
    "\n",
    "We perform **stratified sampling**, ensuring that the class distribution is preserved across all splits. This is crucial for imbalanced datasets like Cora.\n",
    "\n",
    "The typical split used in GNN literature is:\n",
    "\n",
    "- 50-60% for training + validation\n",
    "- 30-40% for testing\n",
    "- Of the training + validation, 20% used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5d9f7-7a8a-40b2-ba9e-6b727259fb67",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use scikit-learn's function `train_test_split` to divide the data into train, test, and validation splits according to the partition sizes specified in the `create_train_val_test_split` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce03338-0943-4228-9ca5-149dc2256471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(num_nodes, y_labels, test_size=0.3, val_size=0.2, random_state=42):\n",
    "    \"\"\"Create stratified train/validation/test splits.\"\"\"\n",
    "    \n",
    "    node_indices = np.arange(num_nodes)\n",
    "    \n",
    "    # Split into train+val and test\n",
    "    train_val_indices, test_indices = ...                 # Divide the data into training/validation and test splits, 1 line\n",
    "    \n",
    "    # Split train+val into train and val\n",
    "    train_indices, val_indices = ...                      # Divide the training/validation data into training and validation splits, 1 line\n",
    "    \n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "# Apply split\n",
    "train_indices, val_indices, test_indices = create_train_val_test_split(\n",
    "    num_nodes, y_labels, test_size=0.3, val_size=0.2\n",
    ")\n",
    "\n",
    "# Report stats\n",
    "print(\"=== Dataset Split Statistics ===\")\n",
    "print(f\"Total nodes:     {num_nodes}\")\n",
    "print(f\"Train nodes:     {len(train_indices)}\")\n",
    "print(f\"Validation nodes:{len(val_indices)}\")\n",
    "print(f\"Test nodes:      {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe78de1-e5b5-46f1-b53a-90c46239d48e",
   "metadata": {},
   "source": [
    "## Visualizing Data Splits with t-SNE\n",
    "\n",
    "We now project the node features into 2D using **t-SNE**, a nonlinear dimensionality reduction method that preserves local structure. We'll color each node by its **class**, and use different markers to distinguish between the **training**, **validation**, and **test** sets.\n",
    "\n",
    "This helps us answer questions like:\n",
    "- Do classes cluster well in feature space?\n",
    "- Are splits representative of the class distribution?\n",
    "- How much overlap exists between regions used for training and those for testing?\n",
    "\n",
    "**Note**: t-SNE is unsupervised and nonlinear, so results may vary depending on parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538cfa6-5d16-4fea-995d-0797bfe6bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_with_splits(X_features, y_labels, train_idx, val_idx, test_idx, title=\"t-SNE of Node Features\"):\n",
    "    \"\"\"Visualize 2D t-SNE projection with color by class and split indicators.\"\"\"\n",
    "    # Convert sparse matrix to dense if needed\n",
    "    X_dense = X_features.toarray() if not isinstance(X_features, np.ndarray) else X_features\n",
    "    \n",
    "    # Compute t-SNE projection\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    X_2d = tsne.fit_transform(X_dense)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training nodes\n",
    "    plt.scatter(X_2d[train_idx, 0], X_2d[train_idx, 1], c=y_labels[train_idx],\n",
    "                cmap=\"tab10\", s=20, marker='o', alpha=0.8, label=\"Train\")\n",
    "\n",
    "    # Plot validation nodes\n",
    "    plt.scatter(X_2d[val_idx, 0], X_2d[val_idx, 1], c=y_labels[val_idx],\n",
    "                cmap=\"tab10\", s=20, marker='s', alpha=0.8, label=\"Validation\")\n",
    "\n",
    "    # Plot test nodes\n",
    "    plt.scatter(X_2d[test_idx, 0], X_2d[test_idx, 1], c=y_labels[test_idx],\n",
    "                cmap=\"tab10\", s=20, marker='^', alpha=0.8, label=\"Test\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the splits\n",
    "plot_tsne_with_splits(X_normalized, y_labels, train_indices, val_indices, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55047c7-5c9b-42df-a0dc-d58fa954dd85",
   "metadata": {},
   "source": [
    "## Baseline Model: Shallow Encoder\n",
    "\n",
    "Before introducing graph-aware models, we begin with a **shallow embedding-based classifier**. This model:\n",
    "\n",
    "- Learns a trainable embedding vector for each node (like Word2Vec or matrix factorization).\n",
    "- Passes the embedding through a dense layer to predict class logits.\n",
    "\n",
    "### Why Use It?\n",
    "\n",
    "- This model ignores graph structure entirely and treats each node ID as a categorical variable.\n",
    "- It serves as a **non-graph baseline** to measure how much the featureless embedding alone can classify nodes.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "$\\text{Input: Node ID} \\quad \\xrightarrow{\\text{Embedding Lookup}} \\quad \\mathbf{z}_v \\in \\mathbb{R}^d \\quad \\xrightarrow{\\text{Dense Layer}} \\quad \\hat{y}_v$\n",
    "\n",
    "This is a useful sanity check: if a GNN can't outperform this model, it's likely overcomplicated or undertrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6687f2-19fe-44fc-a88d-a279eed67fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowEncoder(Model):\n",
    "    \"\"\"Shallow encoder model that learns node embeddings and classifies directly.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_dim=num_nodes, output_dim=embedding_dim)\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, node_ids):\n",
    "        embeddings = self.embedding(node_ids)  # Shape: (batch_size, embedding_dim)\n",
    "        logits = self.classifier(embeddings)   # Shape: (batch_size, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae84a09-7067-4ba4-b6c0-aa6609df721e",
   "metadata": {},
   "source": [
    "## Training the Shallow Encoder\n",
    "\n",
    "We now train the shallow encoder using:\n",
    "\n",
    "- Sparse categorical cross-entropy loss.\n",
    "- Adam optimizer.\n",
    "- Validation monitoring for overfitting.\n",
    "\n",
    "We also plot the training and validation accuracy over epochs, and finally report test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2b11b-2baf-40aa-95fa-ed3d5f1aefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_shallow_encoder(num_nodes, num_classes, train_indices, val_indices, test_indices, y_labels):\n",
    "    \"\"\"Train and evaluate shallow encoder model.\"\"\"\n",
    "    print(\"\\n=== TRAINING SHALLOW ENCODER ===\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    embedding_dim = 32\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 128\n",
    "    epochs = 100\n",
    "    \n",
    "    # Prepare data\n",
    "    train_node_ids = tf.convert_to_tensor(train_indices, dtype=tf.int32)\n",
    "    train_labels = tf.convert_to_tensor(y_labels[train_indices], dtype=tf.int32)\n",
    "    val_node_ids = tf.convert_to_tensor(val_indices, dtype=tf.int32)\n",
    "    val_labels = tf.convert_to_tensor(y_labels[val_indices], dtype=tf.int32)\n",
    "    test_node_ids = tf.convert_to_tensor(test_indices, dtype=tf.int32)\n",
    "    test_labels = tf.convert_to_tensor(y_labels[test_indices], dtype=tf.int32)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = ShallowEncoder(num_nodes, embedding_dim, num_classes)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        x=train_node_ids,\n",
    "        y=train_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(val_node_ids, val_labels),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(test_node_ids, test_labels, verbose=0)\n",
    "    print(f\"Shallow Encoder Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Shallow Encoder Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "# Run training\n",
    "shallow_encoder_accuracy = train_shallow_encoder(\n",
    "    num_nodes, num_classes=len(np.unique(y_labels)),\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    test_indices=test_indices,\n",
    "    y_labels=y_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3538c8cd-4943-4488-8fed-f30d73cbcf04",
   "metadata": {},
   "source": [
    "## Node2Vec: Structure-Aware Node Embeddings\n",
    "\n",
    "While the shallow encoder ignores the graph, **Node2Vec** leverages it by performing random walks to learn structural embeddings.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Nodes are embedded based on their **co-occurrence in random walks** — analogous to how Word2Vec learns embeddings from word contexts.\n",
    "- Each walk explores local neighborhoods using **biased sampling** that can interpolate between:\n",
    "  - **Breadth-first (BFS)**: captures local similarity (homophily).\n",
    "  - **Depth-first (DFS)**: captures structural roles (heterophily).\n",
    "\n",
    "In this notebook, we implement an unweighted version of Node2Vec using uniform random walks.\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "Node2Vec introduces graph structure into learning, but:\n",
    "- It is **unsupervised**: no label information is used during embedding training.\n",
    "- It is **transductive**: embeddings are learned only for the nodes seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d4507-01cb-4442-8a6c-0a12777429bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_walks(graph_object, num_walks=10, walk_length=20):\n",
    "    \"\"\"Generate random walks for Node2Vec training.\"\"\"\n",
    "    walks = []\n",
    "    nodes = list(graph_object)\n",
    "    \n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)\n",
    "        for start_node in nodes:\n",
    "            walk = [start_node]\n",
    "            while len(walk) < walk_length:\n",
    "                current_node = walk[-1]\n",
    "                neighbors = graph_object.children(current_node)\n",
    "                if neighbors:\n",
    "                    walk.append(random.choice(neighbors))\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(walk)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefe1ec-93f7-4b33-a305-b4562c5b52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(walks, window_size=5):\n",
    "    \"\"\"Generate skipgram training pairs from random walks.\"\"\"\n",
    "    pairs = []\n",
    "    for walk in walks:\n",
    "        for i, center_node in enumerate(walk):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(walk), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    pairs.append((center_node, walk[j]))\n",
    "    return pairs\n",
    "\n",
    "def get_negative_samples(vocab_size, exclude, k=5):\n",
    "    \"\"\"Sample negative nodes for contrastive learning.\"\"\"\n",
    "    negatives = []\n",
    "    while len(negatives) < k:\n",
    "        candidate = random.randint(0, vocab_size - 1)\n",
    "        if candidate != exclude:\n",
    "            negatives.append(candidate)\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16318dfb-c7aa-4d3f-8d3d-a19330c1e9ee",
   "metadata": {},
   "source": [
    "## Training Node2Vec Embeddings\n",
    "\n",
    "We now train node embeddings using the **skip-gram with negative sampling** objective, inspired by Word2Vec.\n",
    "\n",
    "### Objective\n",
    "\n",
    "We maximize the likelihood of observing a context node $j$ given a center node $i$, while minimizing the likelihood of randomly sampled negative nodes:\n",
    "\n",
    "$\n",
    "\\log \\sigma(\\mathbf{z}_i^\\top \\mathbf{z}_j) + \\sum_{k=1}^K \\mathbb{E}_{n_k \\sim P_n} \\left[ \\log \\sigma(-\\mathbf{z}_i^\\top \\mathbf{z}_{n_k}) \\right]\n",
    "$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $P_n$ is the negative sampling distribution.\n",
    "\n",
    "We implement this training loop in TensorFlow using random walks and skip-gram pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ad59a-3e89-4f4f-8a5f-912a9a3e58c2",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's train node2vec using the helper functions we defined in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdc1eb-82c8-44c7-bcf6-909ac1ef7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node2vec_embeddings(graph_object, num_nodes, num_walks=10, walk_length=20, \n",
    "                              embedding_dim=64, learning_rate=0.01, num_epochs=5, \n",
    "                              batch_size=256, neg_samples=5):\n",
    "    \"\"\"Train Node2Vec embeddings using skip-gram with negative sampling.\"\"\"\n",
    "    print(\"\\n=== TRAINING NODE2VEC EMBEDDINGS ===\")\n",
    "    \n",
    "    # Generate training data\n",
    "    print(\"Generating random walks...\")\n",
    "    walks = ...                                                  # Generate random walks, 1 line\n",
    "    pairs = ...                                                  # Generate skipgrams, 1 line\n",
    "    print(f\"Generated {len(pairs)} training pairs\")\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    node_embeddings = tf.Variable(tf.random.normal([num_nodes, embedding_dim]), trainable=True)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Training step function\n",
    "    @tf.function\n",
    "    def train_step(center_ids, positive_ids, negative_ids):\n",
    "        with tf.GradientTape() as tape:\n",
    "            center_embed = tf.nn.embedding_lookup(node_embeddings, center_ids)     # [B, D]\n",
    "            positive_embed = tf.nn.embedding_lookup(node_embeddings, positive_ids) # [B, D]\n",
    "            negative_embed = tf.nn.embedding_lookup(node_embeddings, negative_ids) # [B, K, D]\n",
    "            \n",
    "            # Positive scores\n",
    "            positive_scores = tf.reduce_sum(center_embed * positive_embed, axis=1)\n",
    "            positive_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.ones_like(positive_scores), logits=positive_scores\n",
    "            )\n",
    "            \n",
    "            # Negative scores\n",
    "            negative_scores = tf.einsum('ij,ikj->ik', center_embed, negative_embed)  # [B, K]\n",
    "            negative_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.zeros_like(negative_scores), logits=negative_scores\n",
    "            )\n",
    "            negative_loss = tf.reduce_sum(negative_loss, axis=1)\n",
    "            \n",
    "            total_loss = tf.reduce_mean(positive_loss + negative_loss)\n",
    "\n",
    "        gradients = tape.gradient(total_loss, [node_embeddings])\n",
    "        optimizer.apply_gradients(zip(gradients, [node_embeddings]))\n",
    "        return total_loss\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Training Node2Vec embeddings...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(pairs)\n",
    "        batches = [pairs[i:i+batch_size] for i in range(0, len(pairs), batch_size)]\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch in batches:\n",
    "            center_ids = np.array([center for center, _ in batch])\n",
    "            positive_ids = np.array([positive for _, positive in batch])\n",
    "            negative_ids = np.array([\n",
    "                get_negative_samples(num_nodes, exclude=pos, k=neg_samples)\n",
    "                for pos in positive_ids\n",
    "            ])\n",
    "            \n",
    "            loss = train_step(center_ids, positive_ids, negative_ids)\n",
    "            epoch_losses.append(loss.numpy())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {np.mean(epoch_losses):.4f}\")\n",
    "    \n",
    "    return node_embeddings.numpy()\n",
    "\n",
    "# Train embeddings\n",
    "node2vec_embeddings = train_node2vec_embeddings(graph_object, num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335bdcb-4c40-4a12-b3b7-af1619a4a437",
   "metadata": {},
   "source": [
    "## Evaluating Node2Vec Embeddings\n",
    "\n",
    "Once the Node2Vec embeddings are trained, we use them to classify nodes using **logistic regression**.\n",
    "\n",
    "We compare three configurations:\n",
    "\n",
    "1. **Embeddings Only**: Logistic regression on the learned Node2Vec vectors.\n",
    "2. **Embeddings + Features**: Concatenate original node features with embeddings.\n",
    "3. **Raw Features Only**: Logistic regression directly on the normalized input features.\n",
    "\n",
    "This provides insight into how useful the learned embeddings are relative to raw attributes, and whether combining both improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9427c2-2e47-474a-9df9-09e5d18c23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_node2vec_embeddings(embeddings, X_normalized, train_indices, val_indices, test_indices, y_labels):\n",
    "    \"\"\"Evaluate Node2Vec embeddings on classification task.\"\"\"\n",
    "    print(\"\\n=== EVALUATING NODE2VEC EMBEDDINGS ===\")\n",
    "    \n",
    "    # Train classifier on embeddings only\n",
    "    clf_embeddings = LogisticRegression(max_iter=1000)\n",
    "    clf_embeddings.fit(embeddings[train_indices], y_labels[train_indices])\n",
    "    test_acc_embeddings = clf_embeddings.score(embeddings[test_indices], y_labels[test_indices])\n",
    "    print(f\"Node2Vec Embeddings Test Accuracy: {test_acc_embeddings:.4f}\")\n",
    "    \n",
    "    # Train classifier on combined features (embeddings + original features)\n",
    "    X_combined = np.hstack([embeddings, X_normalized.toarray()])\n",
    "    clf_combined = LogisticRegression(max_iter=1000)\n",
    "    clf_combined.fit(X_combined[train_indices], y_labels[train_indices])\n",
    "    test_acc_combined = clf_combined.score(X_combined[test_indices], y_labels[test_indices])\n",
    "    print(f\"Combined Features Test Accuracy: {test_acc_combined:.4f}\")\n",
    "    \n",
    "    # Train classifier on raw features only\n",
    "    X_dense = X_normalized.toarray()\n",
    "    clf_raw = LogisticRegression(max_iter=1000)\n",
    "    clf_raw.fit(X_dense[train_indices], y_labels[train_indices])\n",
    "    test_acc_raw = clf_raw.score(X_dense[test_indices], y_labels[test_indices])\n",
    "    print(f\"Raw Features Test Accuracy: {test_acc_raw:.4f}\")\n",
    "    \n",
    "    return test_acc_embeddings, test_acc_combined, test_acc_raw\n",
    "\n",
    "# Run evaluation\n",
    "node2vec_acc, combined_acc, raw_features_acc = evaluate_node2vec_embeddings(\n",
    "    node2vec_embeddings, X_normalized, train_indices, val_indices, test_indices, y_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b44ff-f9fb-4bd2-a982-5aeb6c025db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_tsne(X_features, y_labels, title=\"t-SNE of Node Features\"):\n",
    "    \"\"\"Plot t-SNE visualization of node embeddings or features.\"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert sparse matrix to dense if needed\n",
    "    X_dense = X_features.toarray() if not isinstance(X_features, np.ndarray) else X_features\n",
    "    \n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_dense)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_labels, cmap=\"tab10\", s=10, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.colorbar(scatter, label=\"Class ID\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_feature_tsne(node2vec_embeddings, y_labels, title=\"t-SNE of Node2Vec Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd72b7-fd6c-49ba-8e22-0443f1764211",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network (GCN)\n",
    "\n",
    "GCNs directly incorporate the graph structure into the learning process by aggregating feature information from neighboring nodes.\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "Each layer in a GCN performs:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}^{(l+1)} = \\sigma\\left( \\hat{A} \\mathbf{H}^{(l)} \\mathbf{W}^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{A}$ is the **normalized adjacency matrix** (with added self-loops).\n",
    "- $\\mathbf{H}^{(l)}$ is the input to layer $l$ (features for layer 0).\n",
    "- $\\mathbf{W}^{(l)}$ is the trainable weight matrix.\n",
    "- $\\sigma$ is a non-linearity (e.g., ReLU).\n",
    "\n",
    "### Why GCNs?\n",
    "\n",
    "- Unlike Node2Vec, GCNs are **supervised** — they use labels during training.\n",
    "- They generalize classical Laplacian smoothing and message-passing schemes.\n",
    "- Each layer propagates information one hop — a two-layer GCN aggregates 2-hop neighborhoods.\n",
    "\n",
    "In this notebook, we implement a simple two-layer GCN from scratch using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba9218-10dc-4ded-b9b6-99bce397a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency_matrix(adj_matrix):\n",
    "    \"\"\"Normalize adjacency matrix: A_hat = D^(-1/2) * (A + I) * D^(-1/2)\"\"\"\n",
    "    # Add self-loops\n",
    "    adj_with_self_loops = adj_matrix + sp.eye(adj_matrix.shape[0])\n",
    "    \n",
    "    # Compute degree matrix\n",
    "    degree_vector = np.array(adj_with_self_loops.sum(axis=1)).flatten()\n",
    "    degree_inv_sqrt = 1.0 / np.sqrt(degree_vector)\n",
    "    degree_inv_sqrt[np.isinf(degree_inv_sqrt)] = 0.0\n",
    "    degree_inv_sqrt_matrix = sp.diags(degree_inv_sqrt)\n",
    "    \n",
    "    # Normalize: D^(-1/2) * A * D^(-1/2)\n",
    "    adj_normalized = degree_inv_sqrt_matrix @ adj_with_self_loops @ degree_inv_sqrt_matrix\n",
    "    return adj_normalized\n",
    "\n",
    "def sparse_to_tf_sparse_tensor(sparse_matrix):\n",
    "    \"\"\"Convert scipy sparse matrix to TensorFlow SparseTensor.\"\"\"\n",
    "    sparse_coo = sparse_matrix.tocoo()\n",
    "    indices = np.vstack((sparse_coo.row, sparse_coo.col)).T\n",
    "    return tf.SparseTensor(\n",
    "        indices=indices,\n",
    "        values=sparse_coo.data.astype(np.float32),\n",
    "        dense_shape=sparse_coo.shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bc414-b76a-438c-bd63-301ac3402b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Graph Convolutional Layer: H' = σ(A_hat * H * W + H * B)\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_transform = tf.keras.layers.Dense(output_dim, use_bias=False)\n",
    "        self.bias_transform = tf.keras.layers.Dense(output_dim, use_bias=False)\n",
    "\n",
    "    def call(self, features, adj_normalized):\n",
    "        # Graph convolution: A_hat * H * W\n",
    "        transformed_features = self.weight_transform(features)\n",
    "        graph_conv = tf.sparse.sparse_dense_matmul(adj_normalized, transformed_features)\n",
    "        \n",
    "        # Residual connection: H * B\n",
    "        residual = self.bias_transform(features)\n",
    "        \n",
    "        return graph_conv + residual\n",
    "\n",
    "class GCNModel(tf.keras.Model):\n",
    "    \"\"\"Two-layer Graph Convolutional Network.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn_layer1 = GCNLayer(hidden_dim)\n",
    "        self.gcn_layer2 = GCNLayer(num_classes)\n",
    "\n",
    "    def call(self, features, adj_normalized, training=False):\n",
    "        hidden = tf.nn.relu(self.gcn_layer1(features, adj_normalized))\n",
    "        logits = self.gcn_layer2(hidden, adj_normalized)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c57af9-f38a-4a62-b3ec-96b5da4fa85b",
   "metadata": {},
   "source": [
    "## Training the GCN\n",
    "\n",
    "We train the GCN using categorical cross-entropy on the training nodes only.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- The **adjacency matrix is fixed** throughout training.\n",
    "- Supervision is provided only on the training nodes.\n",
    "- Node features are shared across the entire graph and updated globally.\n",
    "- We monitor validation accuracy to track overfitting.\n",
    "\n",
    "We train for a fixed number of epochs and then evaluate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7b1a5-f697-4865-b023-026588420118",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The code below will train our GCN model. We have wrote most of it and left a few gaps for you to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e0e26-4ca1-42cf-8795-98aaf01bf742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gcn_model(X_normalized, adj_matrix, train_indices, val_indices, test_indices, y_labels, num_classes):\n",
    "    \"\"\"Train and evaluate GCN model.\"\"\"\n",
    "    print(\"\\n=== TRAINING GRAPH CONVOLUTIONAL NETWORK ===\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    hidden_dim = 16\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 100\n",
    "    \n",
    "    # Preprocess adjacency and features\n",
    "    adj_normalized = ...                                         # Normalise adjacency matrix, 1 line\n",
    "    adj_tf = ...                                                 # Convert sparse matrix to a sparse tensor, 1 line\n",
    "    X_tf = tf.convert_to_tensor(X_normalized.toarray(), dtype=tf.float32)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_onehot = to_categorical(y_labels, num_classes=num_classes)\n",
    "    y_tf = tf.convert_to_tensor(y_onehot, dtype=tf.float32)\n",
    "    \n",
    "    train_idx_tf = tf.convert_to_tensor(train_indices, dtype=tf.int32)\n",
    "    val_idx_tf = tf.convert_to_tensor(val_indices, dtype=tf.int32)\n",
    "    test_idx_tf = tf.convert_to_tensor(test_indices, dtype=tf.int32)\n",
    "    \n",
    "    # Initialize model\n",
    "    gcn_model = GCNModel(hidden_dim, num_classes)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # For tracking\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = gcn_model(X_tf, adj_tf, training=True)\n",
    "            train_logits = tf.gather(logits, train_idx_tf)\n",
    "            train_labels = tf.gather(y_tf, train_idx_tf)\n",
    "            loss = loss_fn(train_labels, train_logits)\n",
    "        gradients = tape.gradient(loss, gcn_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, gcn_model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_step()\n",
    "        train_losses.append(loss.numpy())\n",
    "        \n",
    "        logits = gcn_model(X_tf, adj_tf, training=False)\n",
    "        val_logits = tf.gather(logits, val_idx_tf)\n",
    "        val_preds = tf.argmax(val_logits, axis=1)\n",
    "        val_true = tf.argmax(tf.gather(y_tf, val_idx_tf), axis=1)\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(val_preds, val_true), tf.float32))\n",
    "        val_accuracies.append(val_acc.numpy())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    test_logits = tf.gather(logits, test_idx_tf)\n",
    "    test_preds = tf.argmax(test_logits, axis=1)\n",
    "    test_true = tf.argmax(tf.gather(y_tf, test_idx_tf), axis=1)\n",
    "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_true), tf.float32))\n",
    "    \n",
    "    print(f\"GCN Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title(\"GCN Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.title(\"GCN Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return gcn_model, test_acc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab930ce0-fa27-46c1-9d39-3e8fc8ea5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model, gcn_test_accuracy = train_gcn_model(\n",
    "    X_normalized, adj_matrix,\n",
    "    train_indices, val_indices, test_indices,\n",
    "    y_labels, num_classes=len(np.unique(y_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195e612-2673-4070-b5aa-2d8724f8fe47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Visualizing Deep Embeddings from GCN\n",
    "\n",
    "To gain insights into what the GCN has learned, we extract the output of the first hidden layer — these are the **latent embeddings** after 1-hop message passing.\n",
    "\n",
    "We then project these embeddings into 2D using **t-SNE** and color each point by its class label.\n",
    "\n",
    "This lets us visually assess:\n",
    "- How well-separated the class clusters are.\n",
    "- Whether the GCN embeddings form more compact or discriminative regions compared to Node2Vec or raw features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3a003-0c27-487a-aac8-dce812ee6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gcn_embeddings(gcn_model, X_tf, adj_tf):\n",
    "    \"\"\"Extract deep embeddings from the first GCN layer.\"\"\"\n",
    "    @tf.function\n",
    "    def get_embeddings():\n",
    "        hidden = gcn_model.gcn_layer1(X_tf, adj_tf)\n",
    "        return tf.nn.relu(hidden)\n",
    "    \n",
    "    return get_embeddings().numpy()\n",
    "\n",
    "# Prepare inputs\n",
    "X_tf = tf.convert_to_tensor(X_normalized.toarray(), dtype=tf.float32)\n",
    "adj_normalized = normalize_adjacency_matrix(adj_matrix)\n",
    "adj_tf = sparse_to_tf_sparse_tensor(adj_normalized)\n",
    "\n",
    "# Extract embeddings and plot\n",
    "gcn_embeddings = extract_gcn_embeddings(gcn_model, X_tf, adj_tf)\n",
    "plot_feature_tsne(gcn_embeddings, y_labels, title=\"t-SNE of GCN Deep Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952f8b2-452a-4090-959a-7e2bccfb2bde",
   "metadata": {},
   "source": [
    "## Final Results Summary\n",
    "\n",
    "We have implemented and evaluated three distinct approaches for node classification:\n",
    "\n",
    "| Model                     | Uses Features | Uses Graph | Supervised | Accuracy (Test) |\n",
    "|--------------------------|---------------|------------|------------|-----------------|\n",
    "| Shallow Encoder          | No            | No         | Yes        | ✓ (baseline)    |\n",
    "| Node2Vec Embeddings      | No            | Yes        | No         | ✓               |\n",
    "| Node2Vec + Features      | Yes           | Yes        | No         | ✓               |\n",
    "| Raw Features Only        | Yes           | No         | Yes        | ✓               |\n",
    "| Graph Convolutional Net  | Yes           | Yes        | Yes        | ✓ (deep GNN)    |\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The **Shallow Encoder** captures label signal indirectly through ID-based embeddings, but lacks generalization.\n",
    "- **Node2Vec** embeddings encode local structure but ignore label supervision during training.\n",
    "- Combining embeddings with features boosts performance — each adds orthogonal information.\n",
    "- **GCN** consistently outperforms other methods by jointly leveraging features, labels, and structure.\n",
    "\n",
    "Let’s now plot the accuracy results for direct visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a0c05-41fd-45b0-8803-6b0bbd001a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "methods = ['Shallow\\nEncoder', 'Node2Vec\\nEmbeddings', 'Combined\\nFeatures',\n",
    "           'Raw\\nFeatures', 'GCN']\n",
    "accuracies = [shallow_encoder_accuracy, node2vec_acc, combined_acc,\n",
    "              raw_features_acc, gcn_test_accuracy]\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])\n",
    "plt.title('Node Classification Performance Comparison')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a220953-1ffc-467c-b0af-5d2a1ef4ff74",
   "metadata": {},
   "source": [
    "## Conclusion and Further Directions\n",
    "\n",
    "In this tutorial, we explored multiple strategies for node classification on the Cora citation network, ranging from simple embedding-based baselines to deep learning on graphs:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Non-graph baselines (Shallow Encoder)** can already capture surprising amounts of signal — always start here.\n",
    "- **Node2Vec** provides a powerful way to learn embeddings from structure alone, but lacks supervision.\n",
    "- Combining **Node2Vec with node features** improves generalization, showing complementarity.\n",
    "- **GCNs** outperform other methods by jointly modeling local structure and features under label supervision.\n",
    "\n",
    "### Extensions and Further Reading\n",
    "\n",
    "This tutorial serves as a foundation. Further extensions may include:\n",
    "\n",
    "- **Inductive Learning**:\n",
    "  - Use **GraphSAGE** to generate embeddings for unseen nodes.\n",
    "- **Attention-based Models**:\n",
    "  - Implement **Graph Attention Networks (GAT)** to weigh neighbors dynamically.\n",
    "- **Deeper Architectures**:\n",
    "  - Explore residual connections and deeper GCN variants (e.g., JK-Nets, GCNII).\n",
    "- **Heterogeneous Graphs**:\n",
    "  - Model graphs with multiple node/edge types (e.g., citation + authorship).\n",
    "- **Scalability**:\n",
    "  - Study sampling-based methods for large graphs (e.g., GraphSAINT, Cluster-GCN).\n",
    "\n",
    "Recommended papers:\n",
    "- Kipf & Welling (2016): [Semi-Supervised Classification with GCNs](https://arxiv.org/abs/1609.02907)\n",
    "- Grover & Leskovec (2016): [Node2Vec](https://arxiv.org/abs/1607.00653)\n",
    "- Hamilton et al. (2017): [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)\n",
    "- Velickovic et al. (2017): [Graph Attention Networks](https://arxiv.org/abs/1710.10903)\n",
    "\n",
    "Thank you for following this tutorial! \n",
    "We hope you have enjoyed COMP9418!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
